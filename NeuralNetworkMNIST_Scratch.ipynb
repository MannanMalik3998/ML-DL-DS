{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkMNIST_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnoLNUuYNX-X",
        "colab_type": "text"
      },
      "source": [
        "***Neural network on MNIST***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQCTB1ytot97",
        "colab_type": "text"
      },
      "source": [
        "## Single Hidden layer\n",
        "\n",
        "**epochs = 1500**\n",
        "\n",
        "110 neurons, lr=0.1\n",
        "Training accuracy :  99.92044550517105\n",
        "Test accuracy :  85.0\n",
        "\n",
        "120 neurons, lr=0.2\n",
        "Training accuracy :  100.0\n",
        "Test accuracy :  89.62962962962962\n",
        "\n",
        "200 neurons, lr=0.3\n",
        "Training accuracy :  100.0\n",
        "Test accuracy :  90.0\n",
        "Elapsed time: 58.53140044212341 on GPU\n",
        "\n",
        "As can be seen from the results, 200 neurons with learning rate of 0.3 was the best choice. It can be observed that model is overfitted to the data hence resulting in greater difference in test and train accuracy.\n",
        "\n",
        "Model gave new results every time for the same combination of neuron and learning rate which is because of the random arrangement of dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfB4e9BxO0TC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3476cded-940b-495e-f13b-a187cd62b882"
      },
      "source": [
        "#Necessary imports\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "#Loading dataset - MNIST\n",
        "X = load_digits()\n",
        "\n",
        "#Converts directly to one_hot_encoding\n",
        "labels = pd.get_dummies(X.target)\n",
        "\n",
        "#Splitting into test train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.data, labels, test_size=0.3, random_state=20)\n",
        "\n",
        "#Defining the functions used in neural network\n",
        "\n",
        "#Activation function\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "#Calculating slope aka derivative\n",
        "def calcSlope(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "#Getting probabilities of all classes i.e. digits(0-9)\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "#Loss function\n",
        "def cross_entropy(y_pred, y):\n",
        "    return (y_pred - y)/y.shape[0]\n",
        "\n",
        "#Calculating error per prediction\n",
        "def error(y_pred, y):\n",
        "    logp = - np.log(y_pred[np.arange(y.shape[0]), y.argmax(axis=1)])\n",
        "    return np.sum(logp)/y.shape[0]\n",
        "\n",
        "class neuralNetMNIST:\n",
        "\n",
        "    #Initializing values of weights and biases\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        \n",
        "        self.x = x\n",
        "\n",
        "        #Tried different combinations of neurons such as 120, 200 \n",
        "        neurons = 200\n",
        "\n",
        "        #Tried different combinations of learning rate such as 0.2, 0.3\n",
        "        self.lr = 0.3\n",
        "        ip_dim = x.shape[1]\n",
        "        op_dim = y.shape[1]\n",
        "\n",
        "        #3 layers\n",
        "\n",
        "        #Input layer\n",
        "        self.w1 = np.random.randn(ip_dim, neurons)\n",
        "        self.b1 = np.zeros((1, neurons))\n",
        "        \n",
        "        #Hidden layer\n",
        "        self.w2 = np.random.randn(neurons, neurons)\n",
        "        self.b2 = np.zeros((1, neurons))\n",
        "        \n",
        "        #Output layer\n",
        "        self.w3 = np.random.randn(neurons, op_dim)\n",
        "        self.b3 = np.zeros((1, op_dim))\n",
        "        self.y = y\n",
        "\n",
        "    def forwardProp(self):\n",
        "\n",
        "        #Activation of first layer\n",
        "        z1 = np.dot(self.x, self.w1) + self.b1 # z = wx + b\n",
        "        self.a1 = sigmoid(z1) # a = sigmoid(z)\n",
        "\n",
        "        #Activation of first layer is passed as input to second layer(hidden layer)\n",
        "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
        "        self.a2 = sigmoid(z2)\n",
        "\n",
        "        #Activation of hidden layer is passed to output layer and the final label is predicted by softmax\n",
        "        z3 = np.dot(self.a2, self.w3) + self.b3\n",
        "        self.a3 = softmax(z3)\n",
        "        \n",
        "    def backprop(self):\n",
        "        loss = error(self.a3, self.y) #Calculation of error of predicted label\n",
        "\n",
        "        #Calculating the weight update factor using chain rule\n",
        "        \n",
        "        #Output layer\n",
        "        a3_delta = cross_entropy(self.a3, self.y) \n",
        "        z2_delta = np.dot(a3_delta, self.w3.T)\n",
        "\n",
        "        #hidden layer\n",
        "        a2_delta = z2_delta * calcSlope(self.a2) \n",
        "        z1_delta = np.dot(a2_delta, self.w2.T)\n",
        "\n",
        "        #Input layer\n",
        "        a1_delta = z1_delta * calcSlope(self.a1) # w1\n",
        "\n",
        "        #Weights and biases adjustments\n",
        "        \n",
        "        #Output layer\n",
        "        self.w3 -= self.lr * np.dot(self.a2.T, a3_delta)\n",
        "        self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)\n",
        "\n",
        "        #Hidden layer\n",
        "        self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)\n",
        "        self.b2 -= self.lr * np.sum(a2_delta, axis=0)\n",
        "\n",
        "        #Input layer\n",
        "        self.w1 -= self.lr * np.dot(self.x.T, a1_delta)\n",
        "        self.b1 -= self.lr * np.sum(a1_delta, axis=0)\n",
        "\n",
        "    def predict(self, image):\n",
        "      #Gives prediction on trained network\n",
        "        self.x = image\n",
        "        self.forwardProp()\n",
        "        return self.a3.argmax()#prediction of last layer\n",
        "\n",
        "    def fit(self, epochs):\n",
        "        for i in range(epochs):\n",
        "          self.forwardProp()    \n",
        "          self.backprop()    \n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        acc = 0\n",
        "        for data,label in zip(x, y):\n",
        "            s = model.predict(data)\n",
        "            if s == np.argmax(label):\n",
        "                acc +=1\n",
        "        return acc/len(x)*100\n",
        "      \n",
        "#Initialization  \n",
        "model = neuralNetMNIST(X_train, np.array(y_train))\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "#Training the network\n",
        "model.fit(1500)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\t\n",
        "#Evaluation on dataset  \n",
        "print(\"Training accuracy : \", model.evaluate(X_train, np.array(y_train)))\n",
        "print(\"Test accuracy : \", model.evaluate(X_test, np.array(y_test)))\n",
        "print(elapsed_time)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy :  100.0\n",
            "Test accuracy :  90.0\n",
            "58.53140044212341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4c37QcMq-nn",
        "colab_type": "text"
      },
      "source": [
        "## Double Hidden layer\n",
        "\n",
        "**epochs = 1500**\n",
        "\n",
        "110 neurons, lr=0.1\n",
        "Training accuracy :  98.64757358790771\n",
        "Test accuracy :  90.18518518518519\n",
        "\n",
        "120 neurons, lr=0.2\n",
        "Training accuracy :  100.0\n",
        "Test accuracy :  89.07407407407408\n",
        "\n",
        "200 neurons, lr=0.3\n",
        "Training accuracy :  100.0\n",
        "Test accuracy :  89.81481481481481\n",
        "Elapsed time: 93.31503057479858 on GPU\n",
        "\n",
        "Increasing the number of layers had a very slight effect on the accuracy as compared to a single hidden layer neural network\n",
        "\n",
        "As can be seen from the results, 110 neurons with learning rate of 0.1 was the best choice. It can be observed that model is overfitted to the data hence resulting in greater difference in test and train accuracy.\n",
        "\n",
        "Time taken is increased due to an addition of layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkLrBdLFq_Xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "73e1adc9-6135-4481-ad84-6e6ca145f5dd"
      },
      "source": [
        "#Necessary imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "#Loading dataset - MNIST\n",
        "X = load_digits()\n",
        "\n",
        "#Converts directly to one_hot_encoding\n",
        "labels = pd.get_dummies(X.target)\n",
        "\n",
        "#Splitting into test train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.data, labels, test_size=0.3, random_state=20)\n",
        "\n",
        "#Defining the functions used in neural network\n",
        "\n",
        "#Activation function\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "#Calculating slope aka derivative\n",
        "def calcSlope(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "#Getting probabilities of all classes i.e. digits(0-9)\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "#Loss function\n",
        "def cross_entropy(y_pred, y):\n",
        "    return (y_pred - y)/y.shape[0]\n",
        "\n",
        "#Calculating error per prediction\n",
        "def error(y_pred, y):\n",
        "    logp = - np.log(y_pred[np.arange(y.shape[0]), y.argmax(axis=1)])\n",
        "    return np.sum(logp)/y.shape[0]\n",
        "\n",
        "class neuralNetMNIST:\n",
        "\n",
        "    #Initializing values of weights and biases\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        \n",
        "        self.x = x\n",
        " \n",
        "        neurons = 200\n",
        "\n",
        "        self.lr = 0.3\n",
        "        ip_dim = x.shape[1]\n",
        "        op_dim = y.shape[1]\n",
        "\n",
        "        #3 layers\n",
        "\n",
        "        #Input layer\n",
        "        self.w1 = np.random.randn(ip_dim, neurons)\n",
        "        self.b1 = np.zeros((1, neurons))\n",
        "        \n",
        "        #Hidden layer\n",
        "        self.w2 = np.random.randn(neurons, neurons)\n",
        "        self.b2 = np.zeros((1, neurons))\n",
        "        \n",
        "        #Hidden layer 2\n",
        "        self.w22 = np.random.randn(neurons, neurons)\n",
        "        self.b22 = np.zeros((1, neurons))\n",
        "        \n",
        "        #Output layer\n",
        "        self.w3 = np.random.randn(neurons, op_dim)\n",
        "        self.b3 = np.zeros((1, op_dim))\n",
        "        self.y = y\n",
        "\n",
        "    def forwardProp(self):\n",
        "\n",
        "        #Activation of first layer\n",
        "        z1 = np.dot(self.x, self.w1) + self.b1 # z = wx + b\n",
        "        self.a1 = sigmoid(z1) # a = sigmoid(z)\n",
        "\n",
        "        #Activation of first layer is passed as input to second layer(hidden layer)\n",
        "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
        "        self.a2 = sigmoid(z2)\n",
        "\n",
        "        #Activation of first layer is passed as input to second layer(hidden layer)\n",
        "        z22 = np.dot(self.a2, self.w22) + self.b22\n",
        "        self.a22 = sigmoid(z22)\n",
        "\n",
        "        #Activation of hidden layer is passed to output layer and the final label is predicted by softmax\n",
        "        z3 = np.dot(self.a22, self.w3) + self.b3\n",
        "        self.a3 = softmax(z3)\n",
        "        \n",
        "    def backprop(self):\n",
        "        loss = error(self.a3, self.y) #Calculation of error of predicted label\n",
        "\n",
        "        #Calculating the weight update factor using chain rule\n",
        "        \n",
        "        #Output layer\n",
        "        a3_delta = cross_entropy(self.a3, self.y) \n",
        "        z22_delta = np.dot(a3_delta, self.w3.T)\n",
        "\n",
        "        #hidden layer 2\n",
        "        a22_delta = z22_delta * calcSlope(self.a22) \n",
        "        z2_delta = np.dot(a22_delta, self.w22.T)\n",
        "\n",
        "        #hidden layer 1\n",
        "        a2_delta = z22_delta * calcSlope(self.a2) \n",
        "        z1_delta = np.dot(a2_delta, self.w2.T)\n",
        "\n",
        "        #Input layer\n",
        "        a1_delta = z1_delta * calcSlope(self.a1) # w1\n",
        "\n",
        "        #Weights and biases adjustments\n",
        "        \n",
        "        #Output layer\n",
        "        self.w3 -= self.lr * np.dot(self.a22.T, a3_delta)\n",
        "        self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)\n",
        "\n",
        "        #Hidden layer 2\n",
        "        self.w22 -= self.lr * np.dot(self.a2.T, a22_delta)\n",
        "        self.b22 -= self.lr * np.sum(a22_delta, axis=0)\n",
        "\n",
        "        #Hidden layer 1\n",
        "        self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)\n",
        "        self.b2 -= self.lr * np.sum(a2_delta, axis=0)\n",
        "\n",
        "        #Input layer\n",
        "        self.w1 -= self.lr * np.dot(self.x.T, a1_delta)\n",
        "        self.b1 -= self.lr * np.sum(a1_delta, axis=0)\n",
        "\n",
        "    def predict(self, image):\n",
        "      #Gives prediction on trained network\n",
        "        self.x = image\n",
        "        self.forwardProp()\n",
        "        return self.a3.argmax()#prediction of last layer\n",
        "\n",
        "    def fit(self, epochs):\n",
        "        for i in range(epochs):\n",
        "          self.forwardProp()    \n",
        "          self.backprop()    \n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        acc = 0\n",
        "        for data,label in zip(x, y):\n",
        "            s = model.predict(data)\n",
        "            if s == np.argmax(label):\n",
        "                acc +=1\n",
        "        return acc/len(x)*100\n",
        "      \n",
        "#Initialization  \n",
        "model = neuralNetMNIST(X_train, np.array(y_train))\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "#Training the network\n",
        "model.fit(1500)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\t\n",
        "#Evaluation on dataset  \n",
        "print(\"Training accuracy : \", model.evaluate(X_train, np.array(y_train)))\n",
        "print(\"Test accuracy : \", model.evaluate(X_test, np.array(y_test)))\n",
        "print(elapsed_time)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy :  100.0\n",
            "Test accuracy :  89.81481481481481\n",
            "93.31503057479858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e3_KvcNqGvs",
        "colab_type": "text"
      },
      "source": [
        "## Removing sigmoid function from a single hidden layer network\n",
        "\n",
        "**epochs = 1500**\n",
        "\n",
        "200 neurons, lr=0.3\n",
        "Training accuracy :  10.182975338106603\n",
        "Test accuracy :  9.25925925925926\n",
        "Time elapsed: 31.159974098205566 on GPU\n",
        "\n",
        "Accuracies dropped alot\n",
        "\n",
        "Due to removal of sigmoid function, the time taken was reduced as compared to the model which used sigmoid activation function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olxiy6sHqDQD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "3fc26f2f-f830-4a0a-c2f7-da63211656bc"
      },
      "source": [
        "#Necessary imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "#Loading dataset - MNIST\n",
        "X = load_digits()\n",
        "\n",
        "#Converts directly to one_hot_encoding\n",
        "labels = pd.get_dummies(X.target)\n",
        "\n",
        "#Splitting into test train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.data, labels, test_size=0.3, random_state=20)\n",
        "\n",
        "#Defining the functions used in neural network\n",
        "\n",
        "#Activation function\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "#Calculating slope aka derivative\n",
        "def calcSlope(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "#Getting probabilities of all classes i.e. digits(0-9)\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "#Loss function\n",
        "def cross_entropy(y_pred, y):\n",
        "    return (y_pred - y)/y.shape[0]\n",
        "\n",
        "#Calculating error per prediction\n",
        "def error(y_pred, y):\n",
        "    logp = - np.log(y_pred[np.arange(y.shape[0]), y.argmax(axis=1)])\n",
        "    return np.sum(logp)/y.shape[0]\n",
        "\n",
        "class neuralNetMNIST:\n",
        "\n",
        "    #Initializing values of weights and biases\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        \n",
        "        self.x = x\n",
        " \n",
        "        neurons = 200\n",
        "\n",
        "        self.lr = 0.3\n",
        "        ip_dim = x.shape[1]\n",
        "        op_dim = y.shape[1]\n",
        "\n",
        "        #3 layers\n",
        "\n",
        "        #Input layer\n",
        "        self.w1 = np.random.randn(ip_dim, neurons)\n",
        "        self.b1 = np.zeros((1, neurons))\n",
        "        \n",
        "        #Hidden layer\n",
        "        self.w2 = np.random.randn(neurons, neurons)\n",
        "        self.b2 = np.zeros((1, neurons))\n",
        "        \n",
        "        #Output layer\n",
        "        self.w3 = np.random.randn(neurons, op_dim)\n",
        "        self.b3 = np.zeros((1, op_dim))\n",
        "        self.y = y\n",
        "\n",
        "    def forwardProp(self):\n",
        "\n",
        "        #Activation of first layer\n",
        "        self.a1 = np.dot(self.x, self.w1) + self.b1 # z = wx + b\n",
        "        # self.a1 = sigmoid(z1) # a = sigmoid(z)\n",
        "\n",
        "        #Activation of first layer is passed as input to second layer(hidden layer)\n",
        "        self.a2 = np.dot(self.a1, self.w2) + self.b2\n",
        "        # self.a2 = sigmoid(z2)\n",
        "\n",
        "        #Activation of hidden layer is passed to output layer and the final label is predicted by softmax\n",
        "        z3 = np.dot(self.a2, self.w3) + self.b3\n",
        "        self.a3 = softmax(z3)\n",
        "        \n",
        "    def backprop(self):\n",
        "        loss = error(self.a3, self.y) #Calculation of error of predicted label\n",
        "\n",
        "        #Calculating the weight update factor using chain rule\n",
        "        \n",
        "        #Output layer\n",
        "        a3_delta = cross_entropy(self.a3, self.y) \n",
        "        z2_delta = np.dot(a3_delta, self.w3.T)\n",
        "\n",
        "        #hidden layer\n",
        "        a2_delta = z2_delta * calcSlope(self.a2) \n",
        "        z1_delta = np.dot(a2_delta, self.w2.T)\n",
        "\n",
        "        #Input layer\n",
        "        a1_delta = z1_delta * calcSlope(self.a1) \n",
        "\n",
        "        #Weights and biases adjustments\n",
        "        \n",
        "        #Output layer\n",
        "        self.w3 -= self.lr * np.dot(self.a2.T, a3_delta)\n",
        "        self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)\n",
        "\n",
        "        #Hidden layer\n",
        "        self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)\n",
        "        self.b2 -= self.lr * np.sum(a2_delta, axis=0)\n",
        "\n",
        "        #Input layer\n",
        "        self.w1 -= self.lr * np.dot(self.x.T, a1_delta)\n",
        "        self.b1 -= self.lr * np.sum(a1_delta, axis=0)\n",
        "\n",
        "    def predict(self, image):\n",
        "      #Gives prediction on trained network\n",
        "        self.x = image\n",
        "        self.forwardProp()\n",
        "        return self.a3.argmax()#prediction of last layer\n",
        "\n",
        "    def fit(self, epochs):\n",
        "        for i in range(epochs):\n",
        "          self.forwardProp()    \n",
        "          self.backprop()    \n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        acc = 0\n",
        "        for data,label in zip(x, y):\n",
        "            s = model.predict(data)\n",
        "            if s == np.argmax(label):\n",
        "                acc +=1\n",
        "        return acc/len(x)*100\n",
        "      \n",
        "#Initialization  \n",
        "model = neuralNetMNIST(X_train, np.array(y_train))\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "#Training the network\n",
        "model.fit(1500)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\t\n",
        "#Evaluation on dataset  \n",
        "print(\"Training accuracy : \", model.evaluate(X_train, np.array(y_train)))\n",
        "print(\"Test accuracy : \", model.evaluate(X_test, np.array(y_test)))\n",
        "print(elapsed_time)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:92: RuntimeWarning: overflow encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training accuracy :  10.182975338106603\n",
            "Test accuracy :  9.25925925925926\n",
            "31.159974098205566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QAaR45Zt2Cq",
        "colab_type": "text"
      },
      "source": [
        "### Self Practice Neural networks on small self made dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvWbPZW70FQA",
        "colab_type": "text"
      },
      "source": [
        "First try"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvE-v9iUsWWD",
        "colab_type": "code",
        "outputId": "34cca80e-557e-45bf-c9f1-1e0063cfbbea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "from numpy import exp, array, random, dot\n",
        "\n",
        "#1 neuron\n",
        "#Definining a class for model\n",
        "class sequential():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        #initialing the seed\n",
        "        random.seed(1)\n",
        "        self.weights = 2 * random.random((3, 1)) - 1 #Initializing weights\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + exp(-x))#Sigmoid function\n",
        "\n",
        "    def calcSlope(self, x):\n",
        "        return x * (1 - x) #Slope \n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.sigmoid(dot(X, self.weights)) #Activation\n",
        "\n",
        "    def predict_class(self, X):\n",
        "        if(self.predict(X)>0.5):\n",
        "          return 1\n",
        "        else:\n",
        "          return 0  \n",
        "    \n",
        "    def train(self, X, y, epochs):\n",
        "\n",
        "        for i in range(epochs): #Iterations aka epochs\n",
        "\n",
        "            y_pred = self.predict(X) # label predicted by network, (single neuron)\n",
        "\n",
        "            error = y - y_pred # Calculating error\n",
        "\n",
        "            weight_update = dot(X.T, error * self.calcSlope(y_pred)) \n",
        "\n",
        "            # Updating weights on the basis of slope, \n",
        "            # each weight gets updated on the basis of its contribution in error\n",
        "\n",
        "            self.weights += weight_update\n",
        "\n",
        "#Dataset\n",
        "X = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
        "y = array([[0, 1, 1, 0]]).T\n",
        "\n",
        "\n",
        "print ('\\n Input:')\n",
        "print(X)\n",
        "\n",
        "print ('\\n Actual Output:')\n",
        "print(y)\n",
        "\n",
        "model = sequential() #Instantiating model\n",
        "\n",
        "print (\"\\nWeights before training: \")\n",
        "print (model.weights)\n",
        "\n",
        "#Training on 10000 epochs\n",
        "model.train(X, y, 5000)\n",
        "\n",
        "print (\"\\nWeights after training: \")\n",
        "print (model.weights)\n",
        "\n",
        "print ('\\n\\n*************************\\nPrediction:')\n",
        "\n",
        "print (\"\\n\\nTesting on [1, 0, 0]:\")\n",
        "print (model.predict_class(array([1, 0, 0]))) \n",
        "\n",
        "print (\"\\n\\nTesting on [0, 0, 1]:\")\n",
        "print (model.predict_class(array([0, 0, 1]))) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Input:\n",
            "[[0 0 1]\n",
            " [1 1 1]\n",
            " [1 0 1]\n",
            " [0 1 1]]\n",
            "\n",
            " Actual Output:\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n",
            "\n",
            "Weights before training: \n",
            "[[-0.16595599]\n",
            " [ 0.44064899]\n",
            " [-0.99977125]]\n",
            "\n",
            "Weights after training: \n",
            "[[ 8.95950703]\n",
            " [-0.20975775]\n",
            " [-4.27128529]]\n",
            "\n",
            "\n",
            "*************************\n",
            "Prediction:\n",
            "\n",
            "\n",
            "Testing on [1, 0, 0]:\n",
            "1\n",
            "\n",
            "\n",
            "Testing on [0, 0, 1]:\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z75T96En0IbO",
        "colab_type": "text"
      },
      "source": [
        "Second Try"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy5xtx0Uzv-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X=np.array([ \n",
        "             [1,0,1,0],\n",
        "             [1,0,1,1],\n",
        "             [0,1,0,1]\n",
        "           ])\n",
        "\n",
        "y=np.array([[1],[1],[0]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fB44V8e9Z6g",
        "colab_type": "code",
        "outputId": "a83c65ae-fdcf-40d5-f300-502725271a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "source": [
        "class sequential2():\n",
        "\n",
        "    def __init__(self):\n",
        "      # initializing the variables\n",
        "        # self.epoch=5000 \n",
        "        self.lr=0.1 \n",
        "#2 neurons\n",
        "        # initializing weight and bias\n",
        "        self.w1=np.random.uniform(size=(X.shape[1],3))\n",
        "        self.b1=np.random.uniform(size=(1,3))\n",
        "        self.w2=np.random.uniform(size=(3,1))\n",
        "        self.b2=np.random.uniform(size=(1,1))\n",
        "\n",
        "        self.z1=0\n",
        "        self.z2=0\n",
        "        self.a1=0\n",
        "        self.a2=0\n",
        "        print('\\nWeights before training:\\nwh:\\n{} \\nwout: \\n{} \\n'.format(self.w1,self.w2))\n",
        "\n",
        "\n",
        "\n",
        "    def sigmoid (self,x):\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "    def calc_slope(self,x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def predict(self,X):\n",
        "        #Forward prop\n",
        "        #First layer\n",
        "        self.z1=np.dot(X,self.w1)\n",
        "        self.z1=self.z1 + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)#activation\n",
        "\n",
        "        #Next layer\n",
        "        self.z2=np.dot(self.a1,self.w2)\n",
        "        self.z2= self.z2+ self.b2\n",
        "        self.a2 =self.sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def predict_class(self, X):\n",
        "        if(self.predict(X)>0.5):\n",
        "          return 1\n",
        "        else:\n",
        "          return 0  \n",
        "\n",
        "    def train(self,X, epoch):\n",
        "\n",
        "      # training the model\n",
        "        for i in range(epoch):\n",
        "\n",
        "            #forward propagation\n",
        "            self.predict(X)\n",
        "\n",
        "            #Backpropagation\n",
        "            \n",
        "            #calculating error\n",
        "            E = y-self.a2\n",
        "\n",
        "            #calculating slopes\n",
        "            l2 = self.calc_slope(self.a2)\n",
        "            l1 = self.calc_slope(self.a1)\n",
        "\n",
        "            d2 = E * l2\n",
        "            e1 = d2.dot(self.w2.T)\n",
        "            d1 = e1 * l1\n",
        "\n",
        "            #Updation of weights and biases with respect to error and learning rate\n",
        "            # T = transpose\n",
        "            self.w2 += self.a1.T.dot(d2) *self.lr\n",
        "            self.b2 += np.sum(d2, axis=0,keepdims=True) *self.lr\n",
        "            self.w1 += X.T.dot(d1) *self.lr\n",
        "            self.b1 += np.sum(d1, axis=0,keepdims=True) *self.lr\n",
        "\n",
        "        print('\\nWeights after training:\\nwh: \\n{}\\nwout: \\n{}'.format(self.w1,self.w2))\n",
        "\n",
        "print ('\\n Input:')\n",
        "print(X)\n",
        "\n",
        "print ('\\n Actual Output:')\n",
        "print(y)\n",
        "\n",
        "model = sequential2()\n",
        "model.train(X,5000)\n",
        "print ('\\n\\n*************************\\nPrediction:')\n",
        "\n",
        "print(model.predict_class([1,0,1,0]))\n",
        "print(model.predict_class([1,0,1,1]))\n",
        "print(model.predict_class([0,1,0,1]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Input:\n",
            "[[1 0 1 0]\n",
            " [1 0 1 1]\n",
            " [0 1 0 1]]\n",
            "\n",
            " Actual Output:\n",
            "[[1]\n",
            " [1]\n",
            " [0]]\n",
            "\n",
            "Weights before training:\n",
            "wh:\n",
            "[[0.30233257 0.14675589 0.09233859]\n",
            " [0.18626021 0.34556073 0.39676747]\n",
            " [0.53881673 0.41919451 0.6852195 ]\n",
            " [0.20445225 0.87811744 0.02738759]] \n",
            "wout: \n",
            "[[0.14038694]\n",
            " [0.19810149]\n",
            " [0.80074457]] \n",
            "\n",
            "\n",
            "Weights after training:\n",
            "wh: \n",
            "[[ 0.12834561 -1.41338192  1.6931045 ]\n",
            " [ 0.41635588  1.39998964 -1.86176229]\n",
            " [ 0.36482977 -1.1409433   2.28598541]\n",
            " [ 0.32695637  0.86934366 -1.0568335 ]]\n",
            "wout: \n",
            "[[-0.38489259]\n",
            " [-3.07912606]\n",
            " [ 4.84661501]]\n",
            "\n",
            "\n",
            "*************************\n",
            "Prediction:\n",
            "1\n",
            "1\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXcM2EH1IN_j",
        "colab_type": "text"
      },
      "source": [
        "Try 3 (XOR function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7WgeC47GhtT",
        "colab_type": "code",
        "outputId": "0cc3d178-0cab-41bd-d64f-b8bb51cb29aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "import numpy as np\n",
        "def sigmoid (x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        " \n",
        "#Input data\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([ [0],   [1],   [1],   [0]])\n",
        "\n",
        "epochs = 50000 #was giving poor accuracy on smaller epochs\n",
        "LR = .3 # learning rate\n",
        "\n",
        "#Weight initialization \n",
        "w1 = np.random.uniform(size=(2, 3))\n",
        "w2 = np.random.uniform(size=(3, 1))\n",
        "\n",
        "print ('\\n Input:')\n",
        "print(X)\n",
        "\n",
        "print ('\\n Actual Output:')\n",
        "print(y)\n",
        "\n",
        "print('\\nWeights before training:\\nwh:\\n{} \\nwout: \\n{} \\n'.format(w1,w2))\n",
        "# 2 neurons\n",
        "for i in range(epochs):\n",
        " \n",
        "    # Forward propagation\n",
        "    a = sigmoid(np.dot(X, w1))\n",
        "    y_pred = np.dot(a, w2)\n",
        "    \n",
        "    # Calculate error\n",
        "    e = y - y_pred\n",
        "  \n",
        "    # Backward Propagation \n",
        "    #Weight updation\n",
        "    d2 = e * LR\n",
        "    w2 += a.T.dot(d2)\n",
        "    d1 = d2.dot(w2.T) * sigmoid(a)\n",
        "    w1 += X.T.dot(d1)\n",
        "\n",
        "\n",
        "print('\\nWeights after training:\\nwh: \\n{}\\nwout: \\n{}'.format(w1,w2))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Input:\n",
            "[[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "\n",
            " Actual Output:\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n",
            "\n",
            "Weights before training:\n",
            "wh:\n",
            "[[0.31342418 0.69232262 0.87638915]\n",
            " [0.89460666 0.08504421 0.03905478]] \n",
            "wout: \n",
            "[[0.16983042]\n",
            " [0.8781425 ]\n",
            " [0.09834683]] \n",
            "\n",
            "\n",
            "Weights after training:\n",
            "wh: \n",
            "[[ 3.03000179e-01 -4.24453501e+02  4.25631797e+02]\n",
            " [ 1.18593982e+00  4.24350419e+02 -4.25734879e+02]]\n",
            "wout: \n",
            "[[3.60822483e-15]\n",
            " [5.12865101e-01]\n",
            " [5.12865101e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZS2PwmjHdT6",
        "colab_type": "code",
        "outputId": "174a2af6-6d49-4be4-be8a-6dcfd13d7042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "a = sigmoid(np.dot([1,1], w1))\n",
        "np.dot(a, w2)# hence 0"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.4864549])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}